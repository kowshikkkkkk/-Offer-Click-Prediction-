# ğŸ¯ Offer Click Prediction


A deep learning solution for predicting customer click-through rates on promotional offers, developed for the Unstop ML Competition.

![Python](https://img.shields.io/badge/python-3670A0?style=flat&logo=python&logoColor=ffdd54)
![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white)
![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=flat&logo=scikit-learn&logoColor=white)

---

The dataset is from the Unstop competition and cannot be shared publicly due to competition rules. 

## ğŸ“‹ Table of Contents
- [Problem Statement](#problem-statement)
- [Solution Overview](#solution-overview)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Model Architecture](#model-architecture)
- [Results](#results)
- [Project Structure](#project-structure)
- [Technologies Used](#technologies-used)
- [Future Improvements](#future-improvements)
- [License](#license)
- [Contact](#contact)

---

## ğŸ¯ Problem Statement

The goal is to predict which promotional offers customers are most likely to click on, and rank the top offers for each customer. This helps American Express:
- **Personalize offer recommendations**
- **Improve customer engagement**
- **Optimize marketing ROI**

**Evaluation Metric:** Average Precision (AP) Score

**Dataset:** Parquet files containing customer events, transactions, and offer metadata from Unstop Competition

---

## ğŸ’¡ Solution Overview

This project implements a **deep neural network with embedding layers** for tabular data, combining:
- **Categorical features** â†’ Learned embeddings (Customer ID, Offer ID, etc.)
- **Numerical features** â†’ Normalized continuous values
- **Engineered features** â†’ Time-based and aggregated statistics

### Key Highlights
âœ… Custom PyTorch model with embeddings for high-cardinality categorical features  
âœ… Comprehensive feature engineering (temporal, aggregation, offer validity)  
âœ… Memory-optimized data processing for large datasets  
âœ… Proper train/validation split with model checkpointing  
âœ… Top-7 offer ranking per customer for submission  
âœ… Demo mode for quick testing (~5 minutes)  

---

## ğŸ”§ Features

### Feature Engineering Pipeline
1. **Temporal Features**
   - Days since offer start
   - Days until offer end
   - Offer active duration
   - Event hour and day of week
   - Weekend indicator
   - Offer expiration flag

2. **Aggregated Customer Statistics**
   - Total event count per customer
   - Unique event hours/days patterns
   - Transaction frequency and timing
   - Average transaction hour

3. **Offer Metadata Integration**
   - Discount percentage
   - Offer validity period
   - Offer start/end dates

---

## ğŸš€ Installation

### Prerequisites
- Python 3.8 or higher
- CUDA-capable GPU (optional, but recommended for faster training)

### Setup Instructions
```bash
# 1. Clone the repository
git clone https://github.com/yourusername/click-prediction.git
cd amex-click-prediction

# 2. Create virtual environment (recommended)
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Create necessary directories
mkdir -p data models outputs notebooks
```

---

## ğŸ“Š Usage

### 1. Prepare Data
Place the competition datasets in the `data/` folder:
- `train.parquet`
- `test.parquet`
- `offer_metadata.parquet`
- `additional_event.parquet`
- `additional_transaction.parquet`

### 2. Train Model
```bash
# Run the complete pipeline
python click_prediction_pipeline.py
```

**Expected Runtime:**
- âš¡ **With GPU**: 20-45 minutes (recommended)
- ğŸ¢ **Without GPU**: 1-3 hours

**Quick Demo Mode** (for testing):
```python
# In click_prediction_pipeline.py, line 37, set:
DEMO_MODE = True  # Uses 10% of data with 2 epochs (~5 minutes)
```

### 3. Configuration
Modify hyperparameters in `click_prediction_pipeline.py`:
```python
class Config:
    BATCH_SIZE = 1024
    EPOCHS = 5
    LEARNING_RATE = 0.001
    DROPOUT_RATE = 0.3
    VALIDATION_SPLIT = 0.2
```

### 4. Output Files
- **Trained model**: `models/best_model.pth`
- **Submission file**: `outputs/submission.csv`
- **Training logs**: Console output with epoch-wise metrics

---

## ğŸ—ï¸ Model Architecture
```
ClickNN
â”‚
â”œâ”€â”€ Embedding Layers (Categorical Features)
â”‚   â”œâ”€â”€ Customer ID embedding (id2)
â”‚   â”œâ”€â”€ Offer ID embedding (id3)
â”‚   â”œâ”€â”€ Discount percentage embedding
â”‚   â”œâ”€â”€ Event hour embedding
â”‚   â””â”€â”€ Other categorical embeddings
â”‚
â”œâ”€â”€ Batch Normalization
â”‚   â”œâ”€â”€ Categorical features (after embeddings)
â”‚   â””â”€â”€ Continuous features
â”‚
â”œâ”€â”€ Fully Connected Network
â”‚   â”œâ”€â”€ FC1: (emb_dim + n_continuous) â†’ 256 + ReLU + Dropout(0.3)
â”‚   â”œâ”€â”€ FC2: 256 â†’ 128 + ReLU + Dropout(0.3)
â”‚   â””â”€â”€ FC3: 128 â†’ 1 + Sigmoid
â”‚
â””â”€â”€ Output: Click probability [0, 1]
```

**Key Design Choices:**
- **Embedding dimensions**: `min(50, (n_unique + 1) // 2)` for each categorical feature
- **Loss function**: Binary Cross-Entropy (BCE)
- **Optimizer**: Adam with learning rate 0.001
- **Regularization**: Dropout (0.3) to prevent overfitting
- **Batch Normalization**: Stabilizes training and improves convergence

---

### Performance Insights
- âœ… Model successfully ranks top-7 offers per customer
- âœ… Temporal features significantly improved prediction accuracy
- âœ… Customer transaction history proved to be a strong signal
- âœ… Embedding layers effectively captured high-cardinality categorical relationships

*(Update with your actual scores after training)*

---

## ğŸ“ Project Structure
```
click-prediction/
â”‚
â”œâ”€â”€ data/                              # Dataset files (not tracked in git)
â”‚   â”œâ”€â”€ train.parquet
â”‚   â”œâ”€â”€ test.parquet
â”‚   â”œâ”€â”€ offer_metadata.parquet
â”‚   â”œâ”€â”€ additional_event.parquet
â”‚   â””â”€â”€ additional_transaction.parquet
â”‚
â”œâ”€â”€ models/                            # Saved model checkpoints
â”‚   â””â”€â”€ best_amex_model.pth
â”‚
â”œâ”€â”€ outputs/                           # Prediction results
â”‚   â””â”€â”€ submission.csv
â”‚
â”œâ”€â”€ notebooks/                         # Jupyter notebooks for EDA
â”‚   â””â”€â”€ EDA_and_visualization.ipynb
â”‚
â”œâ”€â”€ click_prediction_pipeline.py  # Main training pipeline
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ .gitignore                         # Git ignore rules
â”œâ”€â”€ README.md                          # Project documentation
â””â”€â”€ LICENSE                            # MIT License
```

---

## ğŸ› ï¸ Technologies Used

- **Deep Learning**: PyTorch 2.0+
- **Data Processing**: Pandas, NumPy
- **Machine Learning**: Scikit-learn
- **Data Format**: Parquet (efficient columnar storage)
- **Version Control**: Git

---

## ğŸ”® Future Improvements

### Model Enhancements
- [ ] Implement K-Fold cross-validation for robust evaluation
- [ ] Add learning rate scheduling (ReduceLROnPlateau)
- [ ] Experiment with transformer-based architectures (TabTransformer)
- [ ] Try attention mechanisms for feature importance

### Feature Engineering
- [ ] Add customer lifetime value (CLV) features
- [ ] Create offer category embeddings
- [ ] Add seasonality features (month, quarter)
- [ ] Customer segmentation features

### Optimization
- [ ] Hyperparameter tuning with Optuna/Ray Tune
- [ ] Ensemble with gradient boosting (LightGBM/XGBoost/CatBoost)
- [ ] Feature selection using SHAP values
- [ ] Mixed precision training (FP16) for faster training

### Deployment
- [ ] Create FastAPI endpoint for real-time predictions
- [ ] Containerize with Docker
- [ ] Add model monitoring and drift detection
- [ ] Create Streamlit dashboard for visualization



```

---

â­ **If you found this project helpful, please consider giving it a star!**

---

*Built with â¤ï¸ for the Unstop ML Competition*
