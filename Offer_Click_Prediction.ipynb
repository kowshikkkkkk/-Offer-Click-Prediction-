{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VupZ2hTcQzKx"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Offer Click Prediction Pipeline\n",
        "Competition: Unstop ML Challenge\n",
        "Author: Your Name\n",
        "Description: End-to-end deep learning pipeline for predicting customer click-through rates on promotional offers\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import average_precision_score\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for the pipeline\"\"\"\n",
        "    # Paths\n",
        "    DATA_DIR = Path('data')\n",
        "    MODEL_DIR = Path('models')\n",
        "    OUTPUT_DIR = Path('outputs')\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 1024\n",
        "    EPOCHS = 5\n",
        "    LEARNING_RATE = 0.001\n",
        "    VALIDATION_SPLIT = 0.2\n",
        "    RANDOM_SEED = 42\n",
        "\n",
        "    # Quick demo mode (for testing/recruiters)\n",
        "    DEMO_MODE = False  # Set to True to run on 10% of data with 2 epochs\n",
        "    DEMO_SAMPLE_SIZE = 0.1\n",
        "    DEMO_EPOCHS = 2\n",
        "\n",
        "    # Model parameters\n",
        "    DROPOUT_RATE = 0.3\n",
        "    EMBEDDING_DIM_MAX = 50\n",
        "\n",
        "    # Device\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    @classmethod\n",
        "    def create_dirs(cls):\n",
        "        \"\"\"Create necessary directories if they don't exist\"\"\"\n",
        "        cls.MODEL_DIR.mkdir(exist_ok=True)\n",
        "        cls.OUTPUT_DIR.mkdir(exist_ok=True)\n"
      ],
      "metadata": {
        "id": "QhFFug2ATndP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Data Loading and Preprocessing\n",
        "\n",
        "def load_data(data_dir: Path) -> tuple:\n",
        "    \"\"\"\n",
        "    Load all required datasets from parquet files\n",
        "\n",
        "    Args:\n",
        "        data_dir: Path to data directory\n",
        "\n",
        "    Returns:\n",
        "        Tuple of dataframes (train, test, offer_meta, add_event, add_trans)\n",
        "    \"\"\"\n",
        "    print(\"Loading datasets...\")\n",
        "\n",
        "    files = {\n",
        "        'train': 'train.parquet',\n",
        "        'test': 'test.parquet',\n",
        "        'offer_meta': 'offer_metadata.parquet',\n",
        "        'add_event': 'additional_event.parquet',\n",
        "        'add_trans': 'additional_transaction.parquet'\n",
        "    }\n",
        "\n",
        "    # Check if all files exist\n",
        "    for name, filename in files.items():\n",
        "        filepath = data_dir / filename\n",
        "        if not filepath.exists():\n",
        "            raise FileNotFoundError(f\"{filename} not found in {data_dir}\")\n",
        "\n",
        "    # Load datasets\n",
        "    train = pd.read_parquet(data_dir / files['train'])\n",
        "    test = pd.read_parquet(data_dir / files['test'])\n",
        "    offer_meta = pd.read_parquet(data_dir / files['offer_meta'])\n",
        "    add_event = pd.read_parquet(data_dir / files['add_event'])\n",
        "    add_trans = pd.read_parquet(data_dir / files['add_trans'])\n",
        "\n",
        "    print(f\"✓ Train shape: {train.shape}, Test shape: {test.shape}\")\n",
        "\n",
        "    # Parse dates\n",
        "    train['event_time'] = pd.to_datetime(train['id4'])\n",
        "    test['event_time'] = pd.to_datetime(test['id4'])\n",
        "    offer_meta['offer_start'] = pd.to_datetime(offer_meta['id12'])\n",
        "    offer_meta['offer_end'] = pd.to_datetime(offer_meta['id13'])\n",
        "    add_event['event_time'] = pd.to_datetime(add_event['id4'])\n",
        "    add_trans['trans_time'] = pd.to_datetime(add_trans['f370'])\n",
        "\n",
        "    return train, test, offer_meta, add_event, add_trans\n",
        "\n",
        "def reduce_memory_usage(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Reduce memory usage by downcasting numeric types\"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'float64':\n",
        "            df[col] = df[col].astype('float32')\n",
        "        elif df[col].dtype == 'int64':\n",
        "            df[col] = df[col].astype('int32')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(f\"✓ Memory reduced: {start_mem:.2f}MB → {end_mem:.2f}MB \"\n",
        "          f\"({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "sCqB7rG5Tnf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "\n",
        "def create_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Create time-based features from offer and event data\"\"\"\n",
        "    # Offer validity features\n",
        "    df['offer_active_days'] = (df['offer_end'] - df['offer_start']).dt.days\n",
        "    df['days_since_offer_start'] = (df['event_time'] - df['offer_start']).dt.days.clip(lower=0)\n",
        "    df['days_until_offer_end'] = (df['offer_end'] - df['event_time']).dt.days.clip(lower=0)\n",
        "    df['offer_expired'] = (df['event_time'] > df['offer_end']).astype(int)\n",
        "\n",
        "    # Time features\n",
        "    df['event_hour'] = df['event_time'].dt.hour\n",
        "    df['event_dayofweek'] = df['event_time'].dt.dayofweek\n",
        "    df['is_weekend'] = (df['event_dayofweek'] >= 5).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_aggregated_features(train: pd.DataFrame, test: pd.DataFrame,\n",
        "                               add_event: pd.DataFrame, add_trans: pd.DataFrame) -> tuple:\n",
        "    \"\"\"Create aggregated customer-level features\"\"\"\n",
        "    print(\"Creating aggregated features...\")\n",
        "\n",
        "    # Event aggregations\n",
        "    event_agg = add_event.groupby('id2').agg(\n",
        "        event_count=('id3', 'count'),\n",
        "        unique_event_hours=('event_time', lambda x: x.dt.hour.nunique()),\n",
        "        unique_event_days=('event_time', lambda x: x.dt.dayofweek.nunique())\n",
        "    ).reset_index()\n",
        "\n",
        "    # Transaction aggregations\n",
        "    trans_agg = add_trans.groupby('id2').agg(\n",
        "        trans_count=('f370', 'count'),\n",
        "        unique_trans_days=('trans_time', lambda x: x.dt.dayofweek.nunique()),\n",
        "        avg_trans_hour=('trans_time', lambda x: x.dt.hour.mean())\n",
        "    ).reset_index()\n",
        "\n",
        "    # Merge to train and test\n",
        "    for df in [train, test]:\n",
        "        df = df.merge(event_agg, on='id2', how='left')\n",
        "        df = df.merge(trans_agg, on='id2', how='left')\n",
        "\n",
        "        # Fill missing values\n",
        "        agg_cols = ['event_count', 'unique_event_hours', 'unique_event_days',\n",
        "                   'trans_count', 'unique_trans_days', 'avg_trans_hour']\n",
        "        df[agg_cols] = df[agg_cols].fillna(0)\n",
        "\n",
        "    return train, test\n",
        "\n",
        "def feature_engineering(train: pd.DataFrame, test: pd.DataFrame,\n",
        "                       offer_meta: pd.DataFrame, add_event: pd.DataFrame,\n",
        "                       add_trans: pd.DataFrame) -> tuple:\n",
        "    \"\"\"Complete feature engineering pipeline\"\"\"\n",
        "    print(\"\\n=== Feature Engineering ===\")\n",
        "\n",
        "    # Merge offer metadata\n",
        "    train = train.merge(\n",
        "        offer_meta[['id3', 'discountpercent', 'offer_start', 'offer_end']],\n",
        "        on='id3', how='left'\n",
        "    )\n",
        "    test = test.merge(\n",
        "        offer_meta[['id3', 'discountpercent', 'offer_start', 'offer_end']],\n",
        "        on='id3', how='left'\n",
        "    )\n",
        "\n",
        "    # Create temporal features\n",
        "    train = create_temporal_features(train)\n",
        "    test = create_temporal_features(test)\n",
        "\n",
        "    # Create aggregated features\n",
        "    train, test = create_aggregated_features(train, test, add_event, add_trans)\n",
        "\n",
        "    # Memory optimization\n",
        "    train = reduce_memory_usage(train)\n",
        "    test = reduce_memory_usage(test)\n",
        "\n",
        "    print(\"✓ Feature engineering complete\\n\")\n",
        "    return train, test\n"
      ],
      "metadata": {
        "id": "s0oBUJN5Tnio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding and Scaling\n",
        "\n",
        "def encode_categorical_features(train: pd.DataFrame, test: pd.DataFrame,\n",
        "                                categorical_cols: list) -> tuple:\n",
        "    \"\"\"Encode categorical features using Label Encoding\"\"\"\n",
        "    print(f\"Encoding {len(categorical_cols)} categorical features...\")\n",
        "\n",
        "    encoders = {}\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        combined = pd.concat([\n",
        "            train[col].astype(str),\n",
        "            test[col].astype(str)\n",
        "        ])\n",
        "        le.fit(combined)\n",
        "\n",
        "        train[col] = le.transform(train[col].astype(str))\n",
        "        test[col] = le.transform(test[col].astype(str))\n",
        "        encoders[col] = le\n",
        "\n",
        "    print(f\"✓ Encoding complete\\n\")\n",
        "    return train, test, encoders\n",
        "\n",
        "def scale_numerical_features(train: pd.DataFrame, test: pd.DataFrame,\n",
        "                             numerical_cols: list) -> tuple:\n",
        "    \"\"\"Standardize numerical features\"\"\"\n",
        "    print(f\"Scaling {len(numerical_cols)} numerical features...\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    train[numerical_cols] = scaler.fit_transform(train[numerical_cols])\n",
        "    test[numerical_cols] = scaler.transform(test[numerical_cols])\n",
        "\n",
        "    print(f\"✓ Scaling complete\\n\")\n",
        "    return train, test, scaler\n"
      ],
      "metadata": {
        "id": "--FAMJoBTnmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch Dataset\n",
        "\n",
        "class ClickDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for click prediction data\"\"\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame, categorical_cols: list,\n",
        "                 numerical_cols: list, target: pd.Series = None):\n",
        "        self.cats = df[categorical_cols].values.astype(np.int64)\n",
        "        self.nums = df[numerical_cols].values.astype(np.float32)\n",
        "        self.y = target.values.astype(np.float32) if target is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cats)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is not None:\n",
        "            return (self.cats[idx], self.nums[idx], self.y[idx])\n",
        "        return (self.cats[idx], self.nums[idx])\n"
      ],
      "metadata": {
        "id": "h2yLvoIBUZzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Model\n",
        "\n",
        "class ClickPredictionNN(nn.Module):\n",
        "    \"\"\"Deep Neural Network with embedding layers for categorical features\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_sizes: list, n_continuous: int,\n",
        "                 dropout_rate: float = 0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding layers for categorical features\n",
        "        self.embeddings = nn.ModuleList([\n",
        "            nn.Embedding(categories, size)\n",
        "            for categories, size in embedding_sizes\n",
        "        ])\n",
        "\n",
        "        emb_dim_sum = sum([size for _, size in embedding_sizes])\n",
        "\n",
        "        # Batch normalization\n",
        "        self.batchnorm_cat = nn.BatchNorm1d(emb_dim_sum)\n",
        "        self.batchnorm_cont = nn.BatchNorm1d(n_continuous)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(emb_dim_sum + n_continuous, 256)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, cat_data, cont_data):\n",
        "        # Process embeddings\n",
        "        embeddings = [emb(cat_data[:, i]) for i, emb in enumerate(self.embeddings)]\n",
        "        x = torch.cat(embeddings, 1)\n",
        "        x = self.batchnorm_cat(x)\n",
        "\n",
        "        # Normalize continuous features\n",
        "        cont_data = self.batchnorm_cont(cont_data)\n",
        "\n",
        "        # Concatenate all features\n",
        "        x = torch.cat([x, cont_data], 1)\n",
        "\n",
        "        # Forward pass\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        output = torch.sigmoid(self.fc3(x))\n",
        "\n",
        "        return output.squeeze()\n"
      ],
      "metadata": {
        "id": "KzXAXvlDUZ1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "def train_model(model: nn.Module, train_loader: DataLoader,\n",
        "                valid_loader: DataLoader, config: Config) -> nn.Module:\n",
        "    \"\"\"Train the neural network\"\"\"\n",
        "    print(f\"=== Training on {config.DEVICE} ===\")\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
        "    model.to(config.DEVICE)\n",
        "\n",
        "    best_ap = 0\n",
        "\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "\n",
        "        for cat, cont, target in train_loader:\n",
        "            cat = cat.to(config.DEVICE)\n",
        "            cont = cont.to(config.DEVICE)\n",
        "            target = target.to(config.DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(cat, cont)\n",
        "            loss = criterion(preds, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * target.size(0)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        y_true, y_pred = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for cat, cont, target in valid_loader:\n",
        "                cat = cat.to(config.DEVICE)\n",
        "                cont = cont.to(config.DEVICE)\n",
        "                target = target.to(config.DEVICE)\n",
        "\n",
        "                preds = model(cat, cont)\n",
        "                y_true.extend(target.cpu().numpy())\n",
        "                y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        avg_precision = average_precision_score(y_true, y_pred)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config.EPOCHS} - Loss: {train_loss:.4f} - AP: {avg_precision:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_precision > best_ap:\n",
        "            best_ap = avg_precision\n",
        "            model_path = config.MODEL_DIR / 'best_model.pth'\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            print(f\"  ✓ Saved best model (AP: {best_ap:.4f})\")\n",
        "\n",
        "    print(f\"\\n✓ Training complete! Best AP: {best_ap:.4f}\\n\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "SAvYljQtUZ4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "\n",
        "def predict(model: nn.Module, test_loader: DataLoader, device: torch.device) -> np.ndarray:\n",
        "    \"\"\"Generate predictions on test set\"\"\"\n",
        "    print(\"Generating predictions...\")\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            if len(batch) == 3:  # Has target\n",
        "                cat, cont, _ = batch\n",
        "            else:  # No target\n",
        "                cat, cont = batch\n",
        "\n",
        "            cat = cat.to(device)\n",
        "            cont = cont.to(device)\n",
        "            preds = model(cat, cont)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "    print(\"✓ Predictions complete\\n\")\n",
        "    return np.array(predictions)\n"
      ],
      "metadata": {
        "id": "Tm8p4XjVUZ7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Pipeline\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"  OFFER CLICK PREDICTION PIPELINE\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Setup\n",
        "    Config.create_dirs()\n",
        "    torch.manual_seed(Config.RANDOM_SEED)\n",
        "    np.random.seed(Config.RANDOM_SEED)\n",
        "\n",
        "    try:\n",
        "        # Load data\n",
        "        train, test, offer_meta, add_event, add_trans = load_data(Config.DATA_DIR)\n",
        "\n",
        "        # Quick demo mode for testing\n",
        "        if Config.DEMO_MODE:\n",
        "            print(f\"\\n⚡ DEMO MODE: Using {Config.DEMO_SAMPLE_SIZE*100}% of data \"\n",
        "                  f\"with {Config.DEMO_EPOCHS} epochs\\n\")\n",
        "            train = train.sample(frac=Config.DEMO_SAMPLE_SIZE, random_state=Config.RANDOM_SEED)\n",
        "            test = test.sample(frac=Config.DEMO_SAMPLE_SIZE, random_state=Config.RANDOM_SEED)\n",
        "\n",
        "        # Feature engineering\n",
        "        train, test = feature_engineering(train, test, offer_meta, add_event, add_trans)\n",
        "\n",
        "        # Define feature columns\n",
        "        categorical_cols = ['id2', 'id3', 'discountpercent', 'event_hour',\n",
        "                          'event_dayofweek', 'is_weekend', 'offer_expired']\n",
        "\n",
        "        exclude_cols = categorical_cols + ['y', 'id1', 'id4', 'event_time',\n",
        "                                          'offer_start', 'offer_end']\n",
        "        numerical_cols = [col for col in train.columns\n",
        "                         if col not in exclude_cols and\n",
        "                         train[col].dtype in [np.float32, np.float64, np.int32, np.int64]]\n",
        "\n",
        "        print(f\"Features: {len(categorical_cols)} categorical + {len(numerical_cols)} numerical\\n\")\n",
        "\n",
        "        # Encode and scale\n",
        "        train, test, encoders = encode_categorical_features(train, test, categorical_cols)\n",
        "        train, test, scaler = scale_numerical_features(train, test, numerical_cols)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            train, train['y'],\n",
        "            test_size=Config.VALIDATION_SPLIT,\n",
        "            random_state=Config.RANDOM_SEED,\n",
        "            stratify=train['y']\n",
        "        )\n",
        "\n",
        "        print(f\"Data split: Train={len(X_train):,} | Validation={len(X_val):,}\\n\")\n",
        "\n",
        "        # Create datasets\n",
        "        train_ds = ClickDataset(X_train, categorical_cols, numerical_cols, y_train)\n",
        "        val_ds = ClickDataset(X_val, categorical_cols, numerical_cols, y_val)\n",
        "        test_ds = ClickDataset(test, categorical_cols, numerical_cols)\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(train_ds, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
        "        val_loader = DataLoader(val_ds, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
        "        test_loader = DataLoader(test_ds, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # Calculate embedding sizes\n",
        "        embedding_sizes = []\n",
        "        for col in categorical_cols:\n",
        "            n_unique = len(encoders[col].classes_)\n",
        "            emb_dim = min(Config.EMBEDDING_DIM_MAX, (n_unique + 1) // 2)\n",
        "            embedding_sizes.append((n_unique, emb_dim))\n",
        "\n",
        "        # Initialize and train model\n",
        "        model = ClickPredictionNN(embedding_sizes, len(numerical_cols), Config.DROPOUT_RATE)\n",
        "        n_params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"Model: {n_params:,} parameters\\n\")\n",
        "\n",
        "        # Use demo epochs if in demo mode\n",
        "        train_config = Config()\n",
        "        if Config.DEMO_MODE:\n",
        "            train_config.EPOCHS = Config.DEMO_EPOCHS\n",
        "\n",
        "        model = train_model(model, train_loader, val_loader, train_config)\n",
        "\n",
        "        # Generate predictions\n",
        "        preds = predict(model, test_loader, Config.DEVICE)\n",
        "        test['pred'] = preds\n",
        "\n",
        "        # Rank offers per customer (top 7)\n",
        "        print(\"Ranking top-7 offers per customer...\")\n",
        "        test = test.sort_values(['id2', 'pred'], ascending=[True, False])\n",
        "        test['rank'] = test.groupby('id2')['pred'].rank(method='first', ascending=False)\n",
        "\n",
        "        # Create submission\n",
        "        submission = test[test['rank'] <= 7][['id1', 'id2', 'id3', 'id5', 'pred', 'rank']]\n",
        "        submission_path = Config.OUTPUT_DIR / 'submission.csv'\n",
        "        submission.to_csv(submission_path, index=False)\n",
        "\n",
        "        print(f\"✓ Submission saved: {submission_path}\")\n",
        "        print(f\"✓ Submission shape: {submission.shape}\")\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\" PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n ERROR: {str(e)}\\n\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "KTNxtw0VUy_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fuzScOegUzGG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}